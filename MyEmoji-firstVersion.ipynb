{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "import imutils\n",
    "import functools\n",
    "import pandas as pd\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceProto = \"PycharmProjects/MyEmoji/opencv_face_detector.pbtxt\"\n",
    "faceModel = \"PycharmProjects/MyEmoji/opencv_face_detector_uint8.pb\"\n",
    "ageProto = \"PycharmProjects/MyEmoji/age_deploy.prototxt\"\n",
    "ageModel = \"PycharmProjects/MyEmoji/age_net.caffemodel\"\n",
    "genderProto = \"PycharmProjects/MyEmoji/gender_deploy.prototxt\"\n",
    "genderModel = \"PycharmProjects/MyEmoji/gender_net.caffemodel\"\n",
    "\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "\n",
    "detector = MTCNN()\n",
    "\n",
    "FaceColor = {\n",
    "    'dark skin tone': [(45, 34, 30), (75, 57, 50)],\n",
    "    'medium-dark skin tone': [(75, 57, 50), (120, 92, 80)],\n",
    "    'medium skin tone': [(120, 92, 80), (180, 138, 120)],\n",
    "    'medium-light skin tone': [(180, 138, 120), (240, 184, 160)],\n",
    "    'light skin tone': [(240, 184, 160), (255, 229, 200)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlightFace(net, image, conf_threshold=0.7):\n",
    "    blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    faceBoxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0, 0, i, 3] * image.shape[1])\n",
    "            y1 = int(detections[0, 0, i, 4] * image.shape[0])\n",
    "            x2 = int(detections[0, 0, i, 5] * image.shape[1])\n",
    "            y2 = int(detections[0, 0, i, 6] * image.shape[0])\n",
    "            faceBoxes.append([x1, y1, x2, y2])\n",
    "\n",
    "    return faceBoxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genderAndAgeDetection(image):\n",
    "    img = image.copy()\n",
    "    faceBoxes = highlightFace(faceNet, img)\n",
    "    padding = 20\n",
    "    for faceBox in faceBoxes:\n",
    "        face = img[max(0, faceBox[1] - padding): min(faceBox[3] + padding, image.shape[0] - 1),\n",
    "                   max(0, faceBox[0] - padding): min(faceBox[2] + padding, image.shape[1] - 1)]\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "        genderNet.setInput(blob)\n",
    "        genderPreds = genderNet.forward()\n",
    "        gender = genderList[genderPreds[0].argmax()]\n",
    "\n",
    "        ageNet.setInput(blob)\n",
    "        agePreds = ageNet.forward()\n",
    "        age = ageList[agePreds[0].argmax()]\n",
    "\n",
    "        return gender, age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSkin(image):\n",
    "    img = image.copy()\n",
    "    # BGR to HSV\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # HSV Thresholds\n",
    "    lower_threshold = np.array([0, 48, 80], dtype=np.uint8)\n",
    "    upper_threshold = np.array([20, 255, 255], dtype=np.uint8)\n",
    "\n",
    "    # channel mask\n",
    "    skinMask = cv2.inRange(img, lower_threshold, upper_threshold)\n",
    "\n",
    "    # cleaning mask by Gaussian Filter\n",
    "    skinMask = cv2.GaussianBlur(skinMask, (3, 3), 0)\n",
    "\n",
    "    # extracting skin color from mask\n",
    "    skin = cv2.bitwise_and(img, img, mask=skinMask)\n",
    "\n",
    "    # return skin image\n",
    "    return cv2.cvtColor(skin, cv2.COLOR_HSV2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeBlack(estimator_labels, estimator_cluster):\n",
    "    # total number of occurrences for each color\n",
    "    occurrence_counter = Counter(estimator_labels)\n",
    "\n",
    "    # Loop through the most common occurring color\n",
    "    for x in occurrence_counter.most_common(len(estimator_cluster)):\n",
    "\n",
    "        # Quick List comprehension to convert each of RBG Numbers to int\n",
    "        color = [int(i) for i in estimator_cluster[x[0]].tolist()]\n",
    "\n",
    "        if Counter(color) == Counter([0, 0, 0]):\n",
    "            # delete the occurrence\n",
    "            del occurrence_counter[x[0]]\n",
    "            # remove the cluster\n",
    "            estimator_cluster = np.delete(estimator_cluster, x[0], 0)\n",
    "            break\n",
    "\n",
    "    return occurrence_counter, estimator_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDominantColor(image, number_of_colors=2):\n",
    "    img = image.copy()\n",
    "\n",
    "    # Convert Image into RGB Colours Space\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # reshape the image to be a list of pixels\n",
    "    img = img.reshape((img.shape[0] * img.shape[1]), 3)\n",
    "\n",
    "    # Initiate KMeans Object\n",
    "    estimator = KMeans(n_clusters=number_of_colors, random_state=0)\n",
    "\n",
    "    # Fit the image\n",
    "    estimator.fit(img)\n",
    "\n",
    "    occurrence, cluster = removeBlack(estimator.labels_, estimator.cluster_centers_)\n",
    "    dominantColors = [int(c) for c in cluster[0]]\n",
    "\n",
    "    def compare(test_list1, test_list2):\n",
    "        return functools.reduce(lambda i, j: i and j, map(lambda m, k: m >= k, test_list1, test_list2), True)\n",
    "\n",
    "    for skin in FaceColor:\n",
    "        if compare(dominantColors, FaceColor[skin][0]) and compare(FaceColor[skin][1], dominantColors):\n",
    "            return dominantColors, skin\n",
    "\n",
    "    return 'dominant color error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender: Female, Age: 15-20 years\n",
      "488    üë©üèΩ\n",
      "Name: emoji, dtype: object\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    image = cv2.imread(\"PycharmProjects/MyEmoji/selfie2.jpg\")\n",
    "\n",
    "    gender, age = genderAndAgeDetection(image)\n",
    "    print(f'Gender: {gender}, Age: {age[1:-1]} years')\n",
    "\n",
    "    image = imutils.resize(image, width=250)\n",
    "    skin = extractSkin(image)\n",
    "\n",
    "    dominantColors = extractDominantColor(skin)\n",
    "\n",
    "    if age[1:-1] == '0-2':\n",
    "        description = 'baby: ' + dominantColors[1]\n",
    "    elif gender == 'Male' and (age[1:-1] == '4-6' or age[1:-1] == '8-12'):\n",
    "        description = 'boy: ' + dominantColors[1]\n",
    "    elif gender == 'Female' and (age[1:-1] == '4-6' or age[1:-1] == '8-12'):\n",
    "        description = 'girl: ' + dominantColors[1]\n",
    "    elif gender == 'Male' and (age[1:-1] == '8-12' or age[1:-1] == '15-20' or age[1:-1] == '25-32'):\n",
    "        description = 'man: ' + dominantColors[1]\n",
    "    elif gender == 'Female' and (age[1:-1] == '8-12' or age[1:-1] == '15-20' or age[1:-1] == '25-32'):\n",
    "        description = 'woman: ' + dominantColors[1]\n",
    "    elif gender == 'Male' and (age[1:-1] == '38-43' or age[1:-1] == '48-53' or age[1:-1] == '60-100'):\n",
    "        description = 'man: ' + dominantColors[1] + ', white hair'\n",
    "    elif gender == 'Female' and (age[1:-1] == '38-43' or age[1:-1] == '48-53' or age[1:-1] == '60-100'):\n",
    "        description = 'woman: ' + dominantColors[1] + ', white hair'\n",
    "\n",
    "    file_path = 'PycharmProjects/MyEmoji/emoji_df.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "    my_emoji = data.loc[(data.name == description)].emoji\n",
    "    print(my_emoji)\n",
    "\n",
    "    cv2.imshow('img', image)\n",
    "    cv2.imshow('skin', skin)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
